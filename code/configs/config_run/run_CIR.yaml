use_ffcv:
  desc: set to True to use ffcv compressed datasets
  value: True
seed:
  desc: set the seed
  value: 0
dataset_splits:
  value: [ "train", "test", "validation"]
dataset_csvs:
  value: ["Ar_ITA_train.csv", "Ar_ITA_test.csv", "Ar_ITA_val.csv"]
dataset_ffcvs:
  value: ["Ar_ITA_train.beton", "Ar_ITA_test.beton", "Ar_ITA_val.beton"]
ffcv_pre_shuffle:
  desc: shuffle dataset before encoding
  value: True
ffcv_path:
  desc: ffcv base directory
  value: /path/to/ffcv
ffcv_order:
  desc: Order used for training. Evaluation is always sequential. RANDOM, QUASI_RANDOM or SEQUENTIAL.
  value: QUASI_RANDOM
ffcv_os_cache:
  desc: Set to true if whole dataset fits in memory (and use RANDOM).
  value: False
ffcv_batches_ahead:
  value: 3

csv_path:
  desc: Action annotation csv
  value: /path/to/csv
csv_shuffle:
  desc: shuffle the dataloader for csv files
  value: True
csv_len:
  desc: only load n samples (useful for debugging). Set to -1 to ignore and use the full dataset.
  value: -1
base_path:
  desc: path where to save runs
  value: /path/to/base/dir

use_text:
  desc: whether to use text features or not
  value: True
CIR_text_dim:
  desc: dimensionality of the features for CIR_text
  value: 512
text_model:
  desc: type of text model used
  value: 'clip-ViT-B-32'
text_dim:
  desc: embedding output dimension of text model used
  value: 512
gen_attn:
  desc: type of attention for recnstruction
  value: 'other'
CIR_text_attn:
  desc: type of attention for building the CIR_text support set, choose between "fixed" and "learned"
  value: 'learned'
CIR_attn:
  desc:
  value: 'fixed'

feat_path:
  desc: Path to flattened features
  value: /path/to/feat/

sample_mode:
  desc: How to sample features for a given action. E.g. start takes the first feature.
  value: customized
n_action_subsample:
  desc: Sequence length of loaded features inside the segment narration.
  value: 3
n_before_after_context:
  desc: Sequence length of loaded features before/after.
  value: 0
resume:
  value: False
last_epoch:
  value: 0
resume_date:
  value: None
save_model:
  value: True
labels:
  desc: Labels returned by the dataloader. Should include action label first, followed by domain labels.
  value: [ 'label_idx', 'scenario_idx', 'location_idx' ]
store_dir:
  desc: Directory where to store the model
  value:  /path/to/save_dir/
model:
  desc: The model to use for extracting video embeddings
  value: 'MLP'
model_prediction:
  desc: The model to use for the final prediction
  value: 'Classifier.logits'
model_types:
  desc: The model to use for training, as defined in models.py
  value: [ 'TextModel', 'MLP', 'CIR', 'Classifier', 'CIR_text']
model_names:
  desc: Names corresponding to the model_types
  value: [ 'TextModel', 'MLP',  'CIR', 'Classifier', 'CIR_text']
model_lrs:
  desc: Learning rate for the models
  value: [ 0.0002, 0.0002, 0.0002, 0.0002, 0.0002 ]
step:
  desc: Whether to do a lr step for the models
  value: [True, True, True, True, True]
model_inputs:
  desc: The model inputs
  value: [ { "text": "data.text" }, { "x": "data.rgb_feat" },  { "x": "MLP.representations", 'target': 'data.target' },{ "x": "MLP.representations","x_mixed":"CIR.feat" },{ "video_features": "MLP.representations", "text_features": "TextModel.text_feat", "target": "data.target" }  ]
model_use_train:
  desc: Models used during training
  value: [ True, True, True, True, True ]
model_use_eval:
  desc: Models used for evaluation
  value: [ False, True, False, True, False ]

bottleneck_dim:
  desc: bottleneck dimension for the learnable cross-attention
  value: 128
mlp_hidden_dims:
  desc: Dimensions of hidden layers in mlp. Needs at least one dimension.
  value: [ 4096, 512 ]
mlp_dropout:
  desc: dropout prob for use in mlp hidden layers
  value: 0.5
prompt_dim:
  desc: dim of the reduced feature vector
  value: 64


loss_types:
  desc: Loss function to use from losses.py
  value: [ "CE",  "CE",  "CIR_text_loss" ]
loss_names:
  desc: Loss function names corresponding the the loss_types
  value: [ "CE", "CIR", "CIR_text_loss"]
loss_inputs:
  desc: Loss function inputs to feed to the loss
  value: [ { 'output': 'Classifier.logits', 'target': 'data.target'},{ 'output': 'Classifier.logits_mixed', 'target': 'data.target' }, { 'output': 'CIR_text.representations', 'target': 'data.target' }, ]

loss_weights:
  desc: Weightings applied to losses when summing them
  value: [ 1.0, 0.5, 1.0, 1.0 ]
LA_tau:
  value: 1.0
weight_decay:
  value: 0.0

boda_upsilon:
  desc: power of the BODA calibrated distance
  value: 1.0
boda_dist:
  desc: distance function between samples
  value: mean
boda_balance_domain:
  desc: Set to true for Eq. 3 and Eq. 4
  value: True
boda_calibrate_distance:
  desc: Set to true for Eq. 4
  value: True
boda_dists:
  desc: choose from mean or mahalanobis. Mahalanobis will probably NAN, as the representation dimension > n samples for some class/domain sets.
  value: mean
boda_level:
  desc: Level of feat we want to apply BODA to.
  value: -1

update_stat_fraction:
  desc: fraction of training dataset used to update stats each epoch
  value: 0.1
update_stat_alpha:
  desc: momentum used in running update of distribution statistics. refers to weight of previous epoch.
  value: 0.5
stat_update_start_epoch:
  desc: epoch to start tracking distribution stats
  value: 100
dist_stats:
  desc: distribution statistics for bode loss. Choose between ["c_d_means", "c_d_vars", "c_d_covs"]
  value: [ "c_d_means" ]

batch_size:
  value: 128

optimizer:
  value: Adam

sch_milestones:
  value: [ 30,40 ]

sch_gamma:
  value: 0.1

epochs:
  value: 50

n_gpu:
  value: 1

feat_dim:
  value: 2304

n_classes:
  value: 60

n_scenarios:
  value: 10

n_locations:
  value: 13

n_workers:
  desc: Number of dataloader workers.
  value: 1

