use_ffcv:
  desc: set to True to use ffcv compressed datasets
  value: True

dataset_splits:
  value: [ "train", "val", "test"]
dataset_csvs:
  value: ['Cooking_train.csv','Cooking_val.csv','Cooking_test.csv']
dataset_ffcvs:
  value: ['train.beton','val.beton','test.beton']

ffcv_pre_shuffle:
  desc: shuffle dataset before encoding
  value: True
ffcv_path:
  desc: ffcv base directory
  value: /user/work/qh22492/ego4d/ffcv_loo/cooking/
ffcv_order:
  desc: Order used for training. Evaluation is always sequential. RANDOM, QUASI_RANDOM or SEQUENTIAL.
  value: QUASI_RANDOM
ffcv_os_cache:
  desc: Set to true if whole dataset fits in memory (and use RANDOM).
  value: False
ffcv_batches_ahead:
  value: 3

csv_path:
  desc: Action annotation csv
  value: /user/work/qh22492/ego4d/leave_one_out_fix_1/
csv_shuffle:
  desc: shuffle the dataloader for csv files
  value: True
csv_len:
  desc: only load n samples (useful for debugging). Set to -1 to ignore and use the full dataset.
  value: -1

feat_path:
  desc: Path to flattened features
  value: /user/work/qh22492/Ego4d/ego4d_data/v1/slowfast8x8_r101_k400

sample_mode:
  desc: How to sample features for a given action. E.g. start takes the first feature.
  value: customized

n_action_subsample:
  desc: Sequence length of loaded features inside the segment narration.
  value: 3

n_before_after_context:
  desc: Sequence length of loaded features before/after.
  value: 0
stride_1:
  desc: Strided indices.
  value: False



labels:
  desc: Labels returned by the dataloader. Should include action label first, followed by domain labels.
  value: [ 'label_idx', 'scenario_idx', 'source_idx' ]

model:
  desc: The model to use. Names as in model.py
  value: MLP

mlp_hidden_dims:
  desc: Dimensions of hidden layers in mlp. Needs at least one dimension.
  value: [ 4096, 4096 ]
mlp_dropout:
  desc: dropout prob for use in mlp hidden layers
  value: 0.5
reduce_dim:
  desc: reducing feature dim before concatenating
  value: False
reduced_dim:
  desc: dim of the reduced feature vector
  value: 1024


trn_scales:
  desc: number of different scales for TRN-multiscale
  value: 3

trn_bottleneck:
  desc: TRN bottleneck dimension
  value: 256
trn_classifier:
  desc: number of different scales for TRN-multiscale
  value: True

losses:
  desc: Loss function to use from losses.py
  value: ["CE"]
loss_weights:
  desc: Weightings applied to losses when summing them
  value: [1.0, 0.1]
LA_tau:
  value: 1.0

boda_upsilon:
  desc: power of the BODA calibrated distance
  value: 1.0
boda_dist:
  desc: distance function between samples
  value: mean
boda_balance_domain:
  desc: Set to true for Eq. 3 and Eq. 4
  value: True
boda_calibrate_distance:
  desc: Set to true for Eq. 4
  value: True
boda_dists:
  desc: choose from mean or mahalanobis. Mahalanobis will probably NAN, as the representation dimension > n samples for some class/domain sets.
  value: mean
boda_level:
  desc: Level of feat we want to apply BODA to.
  value: -1

update_stat_fraction:
  desc: fraction of training dataset used to update stats each epoch
  value: 0.1
update_stat_alpha:
  desc: momentum used in running update of distribution statistics. refers to weight of previous epoch.
  value: 0.5
stat_update_start_epoch:
  desc: epoch to start tracking distribution stats
  value: 100
dist_stats:
  desc: distribution statistics for bode loss.
  # value: ["c_d_means", "c_d_vars", "c_d_covs"]
  value: ["c_d_means"]
mmd_kernel:
  desc: kernel type of MMD, either "gaussian" or "linear"
  # value: ["c_d_means", "c_d_vars", "c_d_covs"]
  value: 'gaussian'

batch_size:
  value: 128

optimizer:
  value: Adam

sch_milestones:
  value: [ 30,40 ]

sch_gamma:
  value: 0.1

lr:
  value: 0.00001

epochs:
  value: 50

n_gpu:
  value: 1

feat_dim:
  value: 2304

n_classes:
  value: 60

n_domains:
  value: 10

n_workers:
  desc: Number of dataloader workers.
  value: 10